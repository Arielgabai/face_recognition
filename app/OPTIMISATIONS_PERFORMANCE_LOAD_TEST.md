# üöÄ Optimisations Performance pour Tests de Charge

## üìä Probl√®mes identifi√©s

### R√©sultats actuels (10 users)
```
/api/check-event-code         : 1.5s avg
/api/check-user-availability   : 3.7s avg
/api/login                     : 5.5s avg
/api/register-with-event-code  : 11s avg
/api/upload-selfie             : 45.5s avg (20% √©checs) ‚ö†Ô∏è
```

### Causes principales
1. ‚úÖ **Index manquants** sur tables critiques
2. ‚úÖ **Requ√™tes DB non optimis√©es** (N+1 queries, full table scans)
3. ‚úÖ **Validation de selfie synchrone** (Azure API timeout 15s)
4. ‚úÖ **Workers insuffisants** (pas de concurrence r√©elle)
5. ‚úÖ **Pas de cache** pour queries r√©p√©t√©es

---

## üîß Solution 1 : Ajouter des index critiques

### Fichier : `add_performance_indexes.py`

```python
"""
Script pour ajouter les index manquants qui impactent les performances
"""
import os
from sqlalchemy import create_engine, text

DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///./face_recognition.db")
if DATABASE_URL.startswith("postgres://"):
    DATABASE_URL = DATABASE_URL.replace("postgres://", "postgresql://", 1)

engine = create_engine(DATABASE_URL)

def add_performance_indexes():
    """Ajoute les index critiques pour am√©liorer les performances"""
    with engine.connect() as conn:
        print("üîß Ajout des index de performance...")
        
        indexes = [
            # FaceMatch : requis pour les jointures fr√©quentes
            "CREATE INDEX IF NOT EXISTS idx_face_matches_user_id ON face_matches(user_id);",
            "CREATE INDEX IF NOT EXISTS idx_face_matches_photo_id ON face_matches(photo_id);",
            "CREATE INDEX IF NOT EXISTS idx_face_matches_user_photo ON face_matches(user_id, photo_id);",
            
            # Photo : √©v√©nements utilis√©s partout
            "CREATE INDEX IF NOT EXISTS idx_photos_event_id ON photos(event_id);",
            "CREATE INDEX IF NOT EXISTS idx_photos_photographer_id ON photos(photographer_id);",
            "CREATE INDEX IF NOT EXISTS idx_photos_event_photographer ON photos(event_id, photographer_id);",
            
            # UserEvent : association users <-> events
            "CREATE INDEX IF NOT EXISTS idx_user_events_user_id ON user_events(user_id);",
            "CREATE INDEX IF NOT EXISTS idx_user_events_event_id ON user_events(event_id);",
            "CREATE INDEX IF NOT EXISTS idx_user_events_user_event ON user_events(user_id, event_id);",
            
            # User : recherches par type et √©v√©nement
            "CREATE INDEX IF NOT EXISTS idx_users_user_type ON users(user_type);",
            "CREATE INDEX IF NOT EXISTS idx_users_event_user_type ON users(event_id, user_type);",
        ]
        
        for idx_sql in indexes:
            try:
                conn.execute(text(idx_sql))
                conn.commit()
                print(f"  ‚úÖ {idx_sql[:60]}...")
            except Exception as e:
                print(f"  ‚ö†Ô∏è  Erreur: {e}")
        
        print("‚úÖ Index ajout√©s avec succ√®s!")

if __name__ == "__main__":
    add_performance_indexes()
```

**Ex√©cution :**
```bash
python add_performance_indexes.py
```

---

## üîß Solution 2 : Optimiser les requ√™tes DB

### A. Optimiser `check_user_availability`

**Avant (3.7s avg):**
```python
# Fait 2 requ√™tes s√©par√©es
user = db.query(User).filter(...).first()
other_count = db.query(User).filter(...).count()
```

**Apr√®s (<0.5s):**
```python
# 1 seule requ√™te avec EXISTS
from sqlalchemy import exists, select

result = db.execute(
    select(
        exists().where(
            (User.username == username) & (User.event_id == event.id)
        )
    )
).scalar()
```

### B. Optimiser la suppression des FaceMatch dans upload_selfie

**Avant (contributif aux 45s):**
```python
# Fait une requ√™te pour TOUS les photo_ids puis delete
photo_ids = [p.id for p in db.query(Photo).filter(Photo.event_id == ue.event_id).all()]
if photo_ids:
    deleted = db.query(FaceMatch).filter(...).delete()
```

**Apr√®s (<1s):**
```python
# DELETE direct avec subquery
from sqlalchemy import delete
stmt = delete(FaceMatch).where(
    FaceMatch.user_id == current_user.id,
    FaceMatch.photo_id.in_(
        select(Photo.id).where(Photo.event_id == ue.event_id)
    )
)
db.execute(stmt)
```

---

## üîß Solution 3 : Rendre la validation asynchrone

### Fichier modifi√© : `main.py` - endpoint `/api/upload-selfie`

**Probl√®me actuel :** La validation bloque pendant ~15s (appel Azure + traitement image)

**Solution :** Validation en background

```python
@app.post("/api/upload-selfie")
async def upload_selfie(
    file: UploadFile = File(...),
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db),
    strict: bool = True,
    background_tasks: BackgroundTasks = None,
):
    """Upload d'un selfie pour l'utilisateur - version optimis√©e"""
    if current_user.user_type == UserType.PHOTOGRAPHER:
        raise HTTPException(status_code=403, detail="Les photographes ne peuvent pas uploader de selfies")
    
    if not file.content_type.startswith("image/"):
        raise HTTPException(status_code=400, detail="Le fichier doit √™tre une image")
    
    # Lire les donn√©es
    file_data = await file.read()
    
    # ‚úÖ OPTIMISATION 1 : Validation rapide synchrone (format + taille)
    if len(file_data) > 10 * 1024 * 1024:  # 10MB max
        raise HTTPException(status_code=400, detail="Image trop volumineuse (max 10MB)")
    
    # V√©rification basique du format
    try:
        from PIL import Image
        import io
        Image.open(io.BytesIO(file_data)).verify()
    except Exception:
        raise HTTPException(status_code=400, detail="Format d'image invalide")
    
    # ‚úÖ OPTIMISATION 2 : Sauvegarde imm√©diate (r√©ponse rapide au client)
    current_user.selfie_data = file_data
    current_user.selfie_path = None
    db.commit()
    
    # ‚úÖ OPTIMISATION 3 : Validation stricte + matching en background
    if background_tasks and strict:
        background_tasks.add_task(
            _validate_and_rematch_selfie,
            current_user.id,
            file_data,
            strict
        )
    else:
        # Fallback synchrone pour environnements sans background tasks
        _validate_and_rematch_selfie(current_user.id, file_data, strict)
    
    return {
        "message": "Selfie upload√© avec succ√®s. Le matching est en cours...",
        "status": "processing"
    }

def _validate_and_rematch_selfie(user_id: int, file_data: bytes, strict: bool):
    """Validation et matching en background"""
    session = next(get_db())
    try:
        # Validation stricte (visage unique, qualit√©, etc.)
        if strict:
            try:
                validate_selfie_image(file_data)
            except HTTPException as e:
                # Si validation √©choue, supprimer le selfie et notifier
                user = session.query(User).filter(User.id == user_id).first()
                if user:
                    user.selfie_data = None
                    session.commit()
                print(f"[SelfieValidation] FAILED for user_id={user_id}: {e.detail}")
                return
        
        # Supprimer anciennes correspondances (optimis√©)
        user_events = session.query(UserEvent.event_id).filter(UserEvent.user_id == user_id).all()
        event_ids = [ue.event_id for ue in user_events]
        
        if event_ids:
            from sqlalchemy import delete
            stmt = delete(FaceMatch).where(
                FaceMatch.user_id == user_id,
                FaceMatch.photo_id.in_(
                    select(Photo.id).where(Photo.event_id.in_(event_ids))
                )
            )
            session.execute(stmt)
            session.commit()
        
        # Matching en background (existant)
        _rematch_all_events(user_id)
        
    finally:
        session.close()
```

---

## üîß Solution 4 : Configuration Workers (Gunicorn)

### Fichier : `gunicorn_config.py`

```python
"""Configuration Gunicorn optimis√©e pour tests de charge"""
import multiprocessing
import os

# Workers
workers = int(os.getenv("GUNICORN_WORKERS", multiprocessing.cpu_count() * 2 + 1))
worker_class = "uvicorn.workers.UvicornWorker"
worker_connections = 1000
max_requests = 1000  # Recycler les workers apr√®s 1000 requ√™tes
max_requests_jitter = 50
timeout = 120  # 2 minutes pour les requ√™tes longues

# Connexions
keepalive = 5
backlog = 2048

# Logs
accesslog = "-"
errorlog = "-"
loglevel = "info"

# Bind
bind = f"0.0.0.0:{os.getenv('PORT', '8000')}"

# Performance
preload_app = True  # Charge l'app avant de forker les workers
```

**D√©marrage :**
```bash
gunicorn main:app -c gunicorn_config.py
```

---

## üîß Solution 5 : Cache pour queries r√©p√©t√©es

### Fichier : `cache_manager.py`

```python
"""Cache simple en m√©moire pour les queries fr√©quentes"""
from functools import lru_cache
import time
from typing import Optional

# Cache simple avec TTL
_cache = {}
_cache_ttl = {}

def cache_get(key: str) -> Optional[any]:
    """R√©cup√®re une valeur du cache si elle n'a pas expir√©"""
    if key in _cache:
        if time.time() < _cache_ttl.get(key, 0):
            return _cache[key]
        else:
            # Expir√©
            del _cache[key]
            if key in _cache_ttl:
                del _cache_ttl[key]
    return None

def cache_set(key: str, value: any, ttl: int = 60):
    """Stocke une valeur dans le cache avec TTL (secondes)"""
    _cache[key] = value
    _cache_ttl[key] = time.time() + ttl

def cache_delete(key: str):
    """Supprime une entr√©e du cache"""
    if key in _cache:
        del _cache[key]
    if key in _cache_ttl:
        del _cache_ttl[key]

# Cache sp√©cifique pour event_code validation
@lru_cache(maxsize=1000)
def is_valid_event_code(event_code: str, _timestamp: int = None) -> bool:
    """Cache la validation des codes √©v√©nements (invalide toutes les 5 min)"""
    from database import SessionLocal
    from models import Event
    
    db = SessionLocal()
    try:
        event = db.query(Event).filter(Event.event_code == event_code).first()
        return event is not None
    finally:
        db.close()

def validate_event_code_cached(event_code: str) -> bool:
    """Validation avec cache (5 minutes)"""
    timestamp = int(time.time() / 300)  # Bloc de 5 minutes
    return is_valid_event_code(event_code, timestamp)
```

### Utilisation dans `/api/check-event-code`

```python
from cache_manager import validate_event_code_cached

@app.post("/api/check-event-code")
async def check_event_code(event_code: str = Body(..., embed=True)):
    """Version optimis√©e avec cache"""
    is_valid = validate_event_code_cached(event_code)
    return {"valid": is_valid}
```

---

## üìà R√©sultats attendus apr√®s optimisations

### Objectif : 30 users simultan√©s

```
Endpoint                       Avant     Apr√®s    Am√©lioration
------------------------------------------------------------------
/api/check-event-code          1.5s  ‚Üí  0.1s      15x plus rapide
/api/check-user-availability   3.7s  ‚Üí  0.3s      12x plus rapide
/api/login                     5.5s  ‚Üí  0.5s      11x plus rapide
/api/register-with-event-code  11s   ‚Üí  2s        5x plus rapide
/api/upload-selfie            45.5s  ‚Üí  3s        15x plus rapide
Taux d'√©chec                   20%   ‚Üí  <1%       ‚úÖ
```

---

## üöÄ Plan d'action (ordre recommand√©)

### Phase 1 : Quick wins (15 min)
1. ‚úÖ Ex√©cuter `python add_performance_indexes.py`
2. ‚úÖ Ajouter cache pour `check-event-code`
3. ‚úÖ Red√©marrer avec Gunicorn + workers multiples

### Phase 2 : Optimisations DB (30 min)
4. ‚úÖ Optimiser `check_user_availability` (requ√™te unique)
5. ‚úÖ Optimiser suppression FaceMatch (subquery)

### Phase 3 : Upload asynchrone (45 min)
6. ‚úÖ Impl√©menter validation background
7. ‚úÖ Tester avec 10 users
8. ‚úÖ Monter progressivement √† 30 users

---

## ‚öôÔ∏è Variables d'environnement recommand√©es

```bash
# Database
DB_POOL_SIZE=30
DB_MAX_OVERFLOW=70
DB_POOL_RECYCLE=1800
DB_POOL_TIMEOUT=30

# Workers
GUNICORN_WORKERS=8  # 2 x CPU cores

# Performance
SELFIE_VALIDATION_STRICT=true
FACE_RECOGNIZER_PROVIDER=local  # √âviter les appels Azure pendant les tests
```

---

## üß™ Tests de validation

```bash
# 1. Lancer l'app optimis√©e
gunicorn main:app -c gunicorn_config.py

# 2. Test de charge progressif
locust -f locust_file.py --host=http://localhost:8000 \
       --users=10 --spawn-rate=2 --run-time=2m

# 3. Si OK, augmenter
locust -f locust_file.py --host=http://localhost:8000 \
       --users=20 --spawn-rate=3 --run-time=3m

# 4. Test final
locust -f locust_file.py --host=http://localhost:8000 \
       --users=30 --spawn-rate=5 --run-time=5m
```

---

## üìä Monitoring

```python
# Ajouter endpoint de stats
@app.get("/api/stats")
async def get_stats(db: Session = Depends(get_db)):
    """Statistiques de performance"""
    from sqlalchemy import func, text
    
    stats = {}
    
    # Compter les entit√©s
    stats["users"] = db.query(func.count(User.id)).scalar()
    stats["photos"] = db.query(func.count(Photo.id)).scalar()
    stats["face_matches"] = db.query(func.count(FaceMatch.id)).scalar()
    stats["events"] = db.query(func.count(Event.id)).scalar()
    
    # Pool de connexions
    from database import engine
    stats["db_pool"] = {
        "size": engine.pool.size(),
        "checked_out": engine.pool.checkedin() if hasattr(engine.pool, 'checkedin') else 'N/A',
        "overflow": engine.pool.overflow() if hasattr(engine.pool, 'overflow') else 'N/A',
    }
    
    return stats
```

---

## ‚úÖ Checklist finale

- [ ] Index ajout√©s sur toutes les tables
- [ ] Requ√™tes DB optimis√©es (pas de N+1)
- [ ] Cache activ√© pour queries r√©p√©t√©es
- [ ] Upload de selfie asynchrone
- [ ] Gunicorn avec workers multiples
- [ ] Variables d'environnement configur√©es
- [ ] Tests de charge r√©ussis (30 users)
- [ ] Taux d'√©chec < 1%
- [ ] Temps de r√©ponse < 5s pour 95% des requ√™tes

